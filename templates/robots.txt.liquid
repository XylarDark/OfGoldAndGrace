# robots.txt for {{ shop.name }}
# Generated by Of Gold And Grace Shopify Theme
#
# This file tells search engines which pages they can and cannot crawl.
# For more information: https://developers.google.com/search/docs/crawling-indexing/robots/introduction
#
# IMPORTANT: Review and customize this file for your specific needs before going live.
# Consider blocking staging/development environments and sensitive admin areas.

# Allow all crawlers by default
User-agent: *

# Allow crawling of main site content
Allow: /

# Block search and filter URLs that may create duplicate content
# Uncomment the following lines if you experience duplicate content issues:
# Disallow: /search
# Disallow: /*?*
# Disallow: /*sort_by=*
# Disallow: /*filter=*

# Block admin and checkout areas (Shopify handles these automatically, but explicit is better)
Disallow: /admin/
Disallow: /cart
Disallow: /checkout
Disallow: /orders/
Disallow: /account
Disallow: /apps/
Disallow: /tools/

# Block password-protected content (Shopify handles this, but explicit)
Disallow: /password

# Block theme assets and development files
Disallow: /assets/
Disallow: /cdn/
Disallow: /*.css$
Disallow: /*.js$

# Block API endpoints
Disallow: /*.js$
Disallow: /*.json$
Disallow: /api/

# Allow access to sitemap (if you have one)
# Allow: /sitemap.xml

# Block common staging/development environments
# Uncomment and modify for your staging setup:
# Disallow: /staging/
# Disallow: /dev/
# Disallow: /test/
# Disallow: /preview/

# Crawl delay (optional - use sparingly as it may slow indexing)
# Crawl-delay: 1

# Sitemap location (uncomment and update if you have a sitemap)
# Sitemap: {{ shop.url }}/sitemap.xml

# Additional user agents can be specified below
# User-agent: Googlebot
# Allow: /

# User-agent: Bingbot
# Allow: /
